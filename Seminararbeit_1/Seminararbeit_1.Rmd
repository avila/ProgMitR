---
title: 'Programmieren mit R: Seminararbeit_1'
author:
- Daniyar Akhmetov
- Marcelo Rainho Avila
- Xuan Son Le (4669361)
date: 'Abgabedatum: 14/11/2017'
output:
  pdf_document:
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
    toc: yes
    toc_depth: 3
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
graphics: yes
lof: no
fontsize: 12
documentclass: article
---
\newpage
# Part I (3 Points)  

## Task  
* *What are the atomic vector types in R? Explain which value they can take and give an example!*
* *What is the difference between generic and atomic vectors?*
* *Explain the following statement: A data frame is a list, but not every list is a data frame.*  

## Answer 
  
1. Atomic vectors are linear vectors (one-dimensional), which consists of values 
of the same type.

**Atomic vectors** | **Values**                          | **Examples**
-------------------|-------------------------------------|---------------
logical            | TRUE/FALSE                          | c(TRUE,FALSE)
integer            | integer numbers                     | c(1L, 10L, 100L)
numeric            | real numbers                        | c(-26, 1.3, -0.25)
complex            | complex numbers a+bi                | c(-2+3i, -4i, 0i)
character          | strings (letters or words)          | c("Hello","2222","How are you")
raw                | raw bytes (as pairs of hex digits)  | as.raw(255) = ff

2. Generic vectors (lists) are also linear vectors (one-dimensional), 
but can contain objects of different types. Example of a list: 
```{r}
x <- list(1:10,"a", c(TRUE,FALSE), c(1 + 2i,3i))
str(x)
```

3. A data frame is a list of atomic vectors of the same length and therefore has a two dimensional structure based on rows and columns. Example of a data frame:
```{r}
df <- data.frame(nickName = c(1, 'B','X'), 
                 hasGlasses = c(FALSE,"No","Yes"),
                 bankAmount = c(800,1.2,-20))
df # the different data types are coerced into one unique data type.
```
Data frame can be seen as a restricted list with equally long atomic vectors, where each vector represents a column of the data frame. A list is a more general case, where the same data structure can hold multiple data types, including functions, data frames and also lists itself. That's why we have the statement.

# Part II (7 Points) 
## Task

* *Explain each line of the following code. In addition, discuss what the output of identical(a, b) will be. Check the help files for the functions set.seed, identical, rnorm and cumsum.*
* *system.time returns the time elapsed for the computation. Explain the differences:*  
```{r, results="hide"}
  # ensure that results from a random generator are reproducible
set.seed(1)
  # generate a vector of normally distributed random numbers
largeVector <- rnorm(100, mean = 5, sd = 10) 
  # define a and b
a <- cumsum(largeVector)[1:100] 
b <- cumsum(largeVector[1:100])
  # is a equal to b?
identical(a, b) 

system.time(cumsum(largeVector)[1:100])
system.time(cumsum(largeVector)[1:100])
```
## Answer  
  
* Because we ran the function set.seed before, the result of randomly choosen largeVector is reproducible for a and b. This means that the values in *a* and *b* are the same, as indicated by the output of `identical(a, b)` 
* A *cumulative sum (cumsum)* is a sequence of partial sums of a given sequence.  
For instance:   
cumsum(largeVector)  
= cumsum(c(k1,k2,k3,...,kn))  
= k1  k1+k2  k1+k2+k3  ...  k1+k2+k3+..+kn  
  
* In a we firstly have a cumulative sum of largeVector and then take the first 100 elements from it:  
a = cumsum(largeVector)[1:100]  
a = (k1  k1+k2  k1+k2+k3 ...  k1+k2+k3+...+kn)[1:100]  
a =  k1  k1+k2  k1+k2+k3 ...  k1+k2+k3+...+k100  
  
* In b we firstly take the first 100 elements from largeVector and then calculate the cumulative sum of these 100 elements.  
b = cumsum(largeVector[1:100])  
b = cumsum(k1 k2 k3 ... k100)  
b = k1  k1+k2  k1+k2+k3 ...  k1+k2+k3+...+k100  

* Now let see why we get different results when applying the function system.time to a and b. On the one hand, `cumsum(largeVector)[1:100]` firstly try to calculate the cumulative sum of the whole largeVector, which contains 10^8 elements. This would take R a while to finish. On the other hand, `(cumsum(largeVector[1:100]))` firstly get the first 100 elements of largeVector and then calculate the cumulative sum of these 100 elements, which runs much faster. That's why we have the difference. 

# Part III (20 Points)

In this section, we aim to construct a model that explains the main factors affecting the probability of survival of the Titanic passengers based on a logistic regression.  Alongside the model construction and interpretation, We also aim to document the steps necessary to conduct such an analysis, such as importing, cleaning and validating the data, as explored in the following sections. 


### Data import  

For this analysis, we used ggplot2, gridExtra, Hmisc and mice libraries, which are not available in the base R packages might have to be installed via the `install.packages()` function. 
```{r import libraries, message = FALSE, results = "hide"}
library(ggplot2)
library(gridExtra)
library(Hmisc)
library(mice)
```

Firstly we need to import the data. In this case, the file was saved in a "Datasets" sub-folder, relative to the current working directory.
```{r loda data}
load(file = "./Datasets/titanic.Rdata")
```

### Descriptive statistics and data validation  

With the data loaded, we can get an overview of of the data with the `describe()` function fromthe package *Hmisc*. 
``` {r ,introDescribe, message=FALSE, results="hide"}
describe(titanic)
```
Due to its lenght, the output is hidden in this report but a few key descriptive statistics can be summerized as following: 

* Regarding the given values:  
    + So we have 1309 observations with 14 features in total.
    + Only 38,2% of 1309 passengers survived. There are more male (843) than female (466) in this data set
    + The average age of these passengers is about 30.
    + The fare varies between 0 and 512 Pre-1970 British Pound, wit mean equals to about 33 and a median of around 14.5 Pounds. 
    + The large majority of passengers embarks on the titanic in Southampton.

* Regarding missing values:  
    + The feature *age* contains about 20% missing values. We assume that Age does have influence on predicting the survival, so we will try to handle these missing values later on.
    + The variable *body* contains about 91% missing values. It is quite a high amount. This variable has to be dropped by the model in any case, since it describes the identification number of bodies found after the accident, which is 100 percent correlated with a vicitm (as long as the data is not missing). 
    + The amounts of missing vales of *fare* and *embarked* are very small. Maybe we would just drop the concerned passengers from the data set or replace the missing values by one certain value.
    + The feature *cabin* also contain a large amount of missing values (1014). While it might contain usefull information about the surival probability, it is not trivial to deal with such a high percentage of missing data and we will drop this variable from our model. 


In order to get a better overview of the data and support the variable selection, in the next following segments, we conduct an explorative analysis through a few plots in order to explore common pattern between the exploratory variables and the target feature *survived*. Firstly, we need to transform the the data type for easier plotting capabilities, as in the following segment.

```{r, fig.width=3,fig.height=1.5}
  # Transform some features, which are given as a labelled vector
titanic$age <- as.numeric(titanic$age)
titanic$fare <- as.numeric(titanic$fare)
titanic$sibsp <- as.numeric(titanic$sibsp)
titanic$parch <- as.numeric(titanic$parch)

  # Some features also need to be factorized, so that numeric values 
  # (for example 1,2,3) should not be treated as number but as category. 
titanic$pclass <- factor(titanic$pclass)
titanic$survived <- factor(titanic$survived,levels = c(0,1),labels = c("No","Yes"))
titanic$sex <- factor(titanic$sex)
titanic$embarked <- factor(titanic$embarked) 
```

We use barplots for categorial features:  
```{r, fig.height=4, fig.width=12, warning=FALSE}
p1 <- ggplot(titanic, aes(x = sex, fill = survived)) + 
    geom_bar() + 
    labs(subtitle = "Survival by Sex", y = "No. of passengers", x = " ") +
    ylim(0,1000)

p2 <- ggplot(titanic, aes(x = pclass,fill = survived)) + 
    geom_bar() + 
    labs(subtitle = "Survival by Passenger class", y = "No. of passengers", x = " ") + 
    ylim(0,1000)
    
p3 <- ggplot(titanic, aes(x = sibsp, fill = survived)) + 
    geom_bar() + 
    labs(subtitle = "Survival by Number of\nSiblings/Spouses Aboard ", 
         y = "No. of passengers", x = " ") + 
    ylim(0,1000) +
    scale_x_continuous(breaks = c(0:8))

p4 <- ggplot(titanic, aes(x = parch, fill = survived)) + geom_bar() +
    labs(subtitle = "Survival by Number of\nParents/Children Aboard ", 
         y = "No. of passengers", x = " ") + 
    scale_x_continuous(breaks = c(0:8))

grid.arrange(p1,p2,p3,p4,ncol = 4)
```

For metric features *age* and *fare* we use boxplots.  
```{r, fig.height=5, fig.width=8, warning=FALSE}
p5 <- ggplot(titanic, aes(y = age, x = survived, fill = survived)) +
    geom_boxplot() +
    labs(subtitle = "Boxplot age", y = "Age", x = " ")

p6 <- ggplot(titanic, aes(y = fare, x = survived, fill = survived)) + 
    geom_boxplot() +
    labs(subtitle = "Boxplot fare", y = "Fares", x = " ") 

grid.arrange(p5,p6,ncol = 2)
```
  
We learn the following things from these plots:  

* *sex*: Females have a much higher chance of survival than males. The *sex* feature is important in our predictions  
* *pclass*: Passengers from 3rd class are less likely to survive than those from 1st and 2rd class.  
* *sibsp* and *parch*: having too many siblings/spouses/parents/children on board or travelling alone would decrease the chance of surviving. Travelling with one or two family members seems to raise the survival chances of the individuals.  
* *fare* contains some outliers, which should be treated later.

It could be also helpful to take a look at the relationship of explanatory features:

```{r}
dropValue <- c("name","cabin","ticket","boat","body","home.dest")
corDataframe <- titanic[ , !(names(titanic) %in% dropValue)] 
corDataframe <- sapply(corDataframe, function(x) x <- as.integer(x))
round(cor(na.omit(corDataframe)), digits = 2)
```
  
+ Our target feature *survived* correlates mostly to *sex* and *pclass*, followed by *fare* and *embarked*. 
+ *pclass* and *fare* correlates strongly negative to each other. So the first class is more expensive than second and third class.
+ *pclass* also correlates quite strongly with *age*. But it could also be a spurious correlation.
+ *sibsp* and *parch* also correlate, since both declare family size

Let us show some multiple relationships graphically:
```{r, fig.height=6, fig.width=10, warning=FALSE}
p7 <- ggplot(titanic,aes(pclass, fill = survived)) + 
    facet_grid( ~ sex) + 
    geom_bar() + labs(subtitle = "Survival by sex and passenger class",
                      y = "No. of passengers")

p8 <- ggplot(subset(titanic, !is.na(embarked)), aes(x = pclass, fill = survived)) +
    facet_grid( ~ embarked) + geom_bar(position = "dodge") + 
    labs(subtitle = "Survival by embarked and passenger class", 
         y = "No. of passengers", x = "") + ylim(0,500)

p9 <- ggplot(titanic, aes(y = fare, x = survived, fill = survived)) +
    facet_grid( ~ pclass) + 
    geom_boxplot() + 
    labs(subtitle = "Survival by fare and passenger class",
         y = "Fare") 

p10 <- ggplot(titanic, aes(y = age, x = survived, fill = survived)) +
    facet_grid( ~ pclass) + geom_boxplot() + 
    labs(subtitle = "Survival by age and passenger class",
         y = "Age") 

grid.arrange(p7, p8, p9, p10, ncol = 2)

```
  
+ Female passengers in the first and second class are mostly survived, meanwhile men in all classes have a similar chance of survival, which is quite low. 
+ A major of passenger embarked in Southampton. Most of them did not survive, especially those in second and third classes. 
+ Unsurprisingly the first class is the most expensive one. The average fare of survived passengers is also higher in the first and second classes. This information could be better explained if we have more information about the relationship between *pclass* and *cabin*.
+ The average age of first class passenger (regardless of survival) is higher than the rest.

That might be enough with plots. Let's move to the next step to choose the most important features for our model.


### Identification of relevant regressors

Since some features do not to contain a lot useful information to our model - or indicates a nerly one to one relationship with the target feature - we drop them from our model. These are: *name, cabin, ticket, boat, body, home.dest*  

* We assume that the name of a passenger does not have any influence on his/her chance of survival. (Although it might be possible to extract some useful information from this variable, such as social class. But for simplicity porpuses, we decided to leave it out of our model)
* We also ignore cabin, since its amount of missing values is too big.  
* Aparently the chance of survival does not depend on ticket and home-destination. The boat number and body identification number are available only after the Titanic had sunk. So they should not be involved in the model as well.  

Let's drop these irrelevant features.  
```{r}
dropValue <- c("name","cabin","ticket","boat","body","home.dest")
titanic <-  titanic[ , !(names(titanic) %in% dropValue)] 

# source: https://stackoverflow.com/questions/4605206
```

#### Missing data
In the following segment, we check for the missing values pattern with help from the library *mice*. The output describes with 1's the available and with 0's the missing values for each variable in an aggregated manner. That is, on the leftmost column we obtain the sum of observations matching the available/missing values of the relative row. Further, on the last row, we obtain the sum of missing values for the relevant variable. With that, we can have a good overview not only about how much data went missing, but also some pattern among missing observations.
  
```{r}
md.pattern(titanic)
```

From the output, we can see that there are 1306 complete observations (within the subset data frame created above, in the full data frame there are more missing values), 1 observation where only the variable *fare* is missing, two observations where *embarked* is missings and 263 where *age* is missing.  Differently to the *fare* and *embarked* variables, 263 (~20%) is a significant amount and care is advised when dealing with such a high proportion of missing data.  Even more, since it is very plausible that the data did not go randomly missing, but a more systematic cause can be underlying the problem.  For instance, it is reasonable that the observations on the deceased passangers is more likely to go missing that on those who survived.  Nonetheless, dropping those observations completely and neglecting the remaining information that these data points would convey, also affecting negatively our model. 



```{r aggregate}
aggregate(titanic[c(2,4:7)],
          by = list(titanic$pclass, titanic$sex),
          FUN = mean,
          na.rm = TRUE,
          na.action = na.pass
)
# source: https://stackoverflow.com/questions/16844613
```
Due to the great age variation among male female passengers in different classes, we replace the `NA` values conditional on their respective class and sex.  This should perform better than naively using the `mean()` or `median()` values.  

```{r replace missing values}
titanic[is.na(titanic$age) &
        titanic$pclass == "1st" &
        titanic$sex == "female", "age"] <- 37.03759

titanic[is.na(titanic$age) &
        titanic$pclass == "2nd" &
        titanic$sex == "female", "age"] <- 27.49919

titanic[is.na(titanic$age) &
        titanic$pclass == "3rd" &
        titanic$sex == "female", "age"] <- 22.18531

titanic[is.na(titanic$age) &
        titanic$pclass == "1st" &
        titanic$sex == "male", "age"] <- 41.02925

titanic[is.na(titanic$age) &
        titanic$pclass == "2nd" &
        titanic$sex == "male", "age"] <- 30.81540

titanic[is.na(titanic$age) &
        titanic$pclass == "3rd" &
        titanic$sex == "male", "age"] <- 25.96227
```

Regarding the other variables, since only very few data is missing, we 
* **fare**: Replace 1 missing value by the mean *fare* in the concerned *pclass*.
```{r}
  # Replace the missing values by the average fare
meanFare <- mean(titanic$fare, na.rm = TRUE)
titanic$fare[is.na(titanic$fare)] <- meanFare
```

* **embarked**: Replace missing value by the most frequent value.
```{r}
summary(titanic$embarked)
titanic$embarked[is.na(titanic$embarked)] <- "Southampton"
```
  
Let's check once again, whether all missing values are handled.  
``` {r}
any(is.na(titanic))
```
  
As we noticed before, travelling alone or with too many family members seems to decrease the chance of survival. Therefore we decide to create a new feature *numberFamlilyMember*, which contains the number of family member on board and a binary feature *anyFamilyMember*, which indicates whether there is any family member on board. Including *numberFamlilyMember* in the model can tell us whether a large family good for survival is or not, etc.

```{r}
 # define function for anyFamilyMember
myFunction <- function(x,y){
    if (x == '0' | y == '0') {
     return("No")
 } else {
     return("Yes") }} # No if x = 0 or y = 0, else Yes

  # apply myFunction to features sibsp and parch
anyFamilyMember <- mapply(myFunction,titanic$sibsp, titanic$parch)
  # create feature anyFamilyMember
titanic <- data.frame(titanic,anyFamilyMember)
  # factorise anyFamilyMember
titanic$anyFamilyMember <- factor(titanic$anyFamilyMember)

  # define function for numberFamilyMember
myFunction1 <- function(x,y){return(x + y)}
  # apply myFunction1 to features sibsp and parch
numberFamilyMember <- mapply(myFunction1,titanic$sibsp, titanic$parch)
  # create feature numberFamilyMember
titanic <- data.frame(titanic,numberFamilyMember)

# source: https://stackoverflow.com/questions/10078211
```
### Fitting a regression model  
Let's create a train and test data set. Now we still have 1309 passengers in total.
```{r}
train <- titanic[1:1100,] # get the first 1000 passengers for training data
test <- titanic[1101:1309,] # the rest passengers for test data
```

Let's fit the logistic regression model. Hier we use the backward selection to keep only relevant regressors. A backward selection removes every regressor from a model step by step and compare the performance of model with and without this regressor.

```{r}
  # Training a model with all filtered features. Hier we use the training data.
model_full <- glm(survived ~., family = binomial, data = train) 
  # Run a backward selection
model <- step(model_full, direction = "backward") 
```

From now we only use the model after backward selection as reference for the next steps. Our model contains from now the following features: *pclass, sex, age, sibsp, embarked, anyFamilyMember*

### Discussion of model fit, e.g. goodness of fit, significance of regressors

Let's take a look at the significance of regressors in our model.
```{r}
summary(model)
```
The p-value, which ranges between 0 and 1, represents the significance of each regressor. The higher p-value is, the more insignificant is the concerned regressor. *sex* has the lowest p-value, which means that sex of the passenger associate strongly with the target feature *survived*. The next significant features are *pclass* and *age*. The rest seems to be higher but still nearby 0.

Let's calculate the Odd-Ratios.
```{r}
exp(model$coef)
```

Being a male will increase the chance of surviving by the factor of 0.068 (which means the chance of surviving would be smaller). Passengers from the first class have more chance for surviving then other classes, which had been estimated before.  

```{r}
anova(model, test = "Chisq")
```

The anova() function allows us to see the difference between the null and residuals deviances. The null deviances is 1497.9, which indicates the performance of the model with only the intercept based on the Chi-Square values. The difference between the null deviance and the residual deviance shows how well the actual model performs against the null model. Again, feature *sex* performs the best regarding the significant improment of the residual deviance, following by *pclass* and *age*. Other features also reduce the residual deviance, eventhough just a bit, and the significances are way worse.  

### Interpreting the model

Let's see how good our model performs regarding the accuracy.
```{r}
  # Predict the test data. The column survived is hidden during the predicting.
test_predict <- predict(model,type = 'response', newdata = test[-2]) 

  # Assign the results into 2 categories of survival.
survived_predict <- ifelse(test_predict > 0.5,"Yes","No") 

  # Create a confusion matrix and print it.
confusion_matrix <- table(test[,2],survived_predict) 
confusion_matrix

  # Calculate the accuracy and print it.
accuracy <- mean(survived_predict == test$survived)*100
print(paste("The model accuracy is", round(accuracy,digits = 2), "%"))
```

There are still several ways to improve the quality of the data set as well as the performance of our predicted model. 

On the one hand, there is a method called k-fold cross validation, which is used to limit problems like overfitting (overfitting: a model which contains too many explanatory variables than necessary). This method partitiones randomly the data set into k equal sized subsets. One of these subsets serves as test data and the rest (k-1) as training data. The cross-validation repeats k-times and returns k different results, which could be averaged to one final result. 
  
We also notice that the feature *name* actually contains some interesting information, which could be relevant to the predicted model. Many names contain such words like "Master", "Dr.", "Major", etc. Maybe it could somehow affect the result of survival. The code below shows how to split the title from the name. But we did not consider it furthermore because this new feature is extremly unbalanced. The major titles are "Mr." and "Mrs.", which actually determine the sex. Maybe it would be helpful to arrange another titles in some groups to balance this feature somehow. 

```{r eval=FALSE, warning=FALSE}
  # Split title from name
nameSplit <- sapply(titanic$name, function(x) strsplit(x, "[,]"))
nameSplit <- sapply(nameSplit, function(x) x <-  x[2])
nameSplit <- sapply(nameSplit, function(x) strsplit(x, "[.]"))
nameTitle <- sapply(nameSplit, function(x) x[1])

  # See all unique titles 
describe(nameTitle)
  # source: https://www.math.ucla.edu/~anderson/rw1001/library/base/html/strsplit.html  
```  